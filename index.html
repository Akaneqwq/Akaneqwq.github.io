<div class="publication">
<picture>
<source srcset="/media/360DVD.avif" type="image/avif" />
<source srcset="/media/360DVD.webp" type="image/webp" />
<img src="/media/360DVD.png" width="180" height="140" alt="360DVD: Controllable Panorama Video Generation with 360-Degree Video Diffusion Model" />
</picture>
<div>
<p class="title">360DVD: Controllable Panorama Video Generation with 360-Degree Video Diffusion Model</p>
<div class="authors">
<b>Qian Wang</b>,
<a href="https://github.com/lwq20020127/" target="_blank">Weiqi Li</a>,
<a href="https://github.com/MC-E" target="_blank">Chong Mou</a>,
<a href="https://cxh0519.github.io/" target="_blank">Xinhua Cheng</a>,
<a href="https://jianzhang.tech/" target="_blank">Jian Zhang</a></div>
<p class="venue">CVPR, 2023</p>
<div class="links">
<span class="link">
<a href="https://arxiv.org/abs/2401.06578" target="_blank">
Paper
</a>
</span>
<span class="link">
<a href="https://akaneqwq.github.io/360DVD/" target="_blank">
Project
</a>
</span>
<span class="link">
<a href="https://github.com/Akaneqwq/360DVD" target="_blank">
Code
</a>
</span>
</div>
<p class="desc">
We propose a controllable panorama video generation pipeline named 360-Degree Video Diffusion model (360DVD) for generating panoramic videos based on the given prompts and motion conditions.
</p>
</div>
</div>

<div class="publication">
<picture>
<source srcset="/media/NTIRE2023.avif" type="image/avif" />
<source srcset="/media/NTIRE2023.webp" type="image/webp" />
<img src="/media/NTIRE2023.png" width="180" height="140" alt="NTIRE 2023 Challenge on 360deg Omnidirectional Image and Video Super-Resolution: Datasets, Methods and Results" />
</picture>
<div>
<p class="title">NTIRE 2023 Challenge on 360deg Omnidirectional Image and Video Super-Resolution: Datasets, Methods and Results</p>
<div class="authors">
Mingdeng Cao,
et al.,
<b>Qian Wang</b>,
et al.,
Bingchun Luo</div>
<p class="venue">CVPR Workshop, 2023</p>
<div class="links">
<span class="link">
<a href="https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Cao_NTIRE_2023_Challenge_on_360deg_Omnidirectional_Image_and_Video_Super-Resolution_CVPRW_2023_paper.html" target="_blank">
Paper
</a>
</span>
</div>
<p class="desc">
We develop a spatial-temporal two-stage model, wherein the first stage is a 4x image super-resolution network, and the second stage is a 4x video super-resolution network.
</p>
</div>
</div>

<div class="publication">
<picture>
<source srcset="/media/CVPR2023-PCFF.avif" type="image/avif" />
<source srcset="/media/CVPR2023-PCFF.webp" type="image/webp" />
<img src="/media/CVPR2023-PCFF.png" width="180" height="140" alt="Panoptic Compositional Feature Field for Editable Scene Rendering with Network-Inferred Labels via Metric Learning" />
</picture>
<div>
<p class="title">Panoptic Compositional Feature Field for Editable Scene Rendering with Network-Inferred Labels via Metric Learning</p>
<div class="authors">
<a href="https://cxh0519.github.io/" target="_blank">Xinhua Cheng</a>,
<a href="https://github.com/yanmin-wu" target="_blank">Yanmin Wu</a>,
<a href="https://mxjia.github.io/" target="_blank">Mengxi Jia</a>,
<b>Qian Wang</b>,
<a href="https://jianzhang.tech/" target="_blank">Jian Zhang</a></div>
<p class="venue">CVPR, 2023</p>
<div class="links">
<span class="link">
<a href="https://openaccess.thecvf.com/content/CVPR2023/html/Cheng_Panoptic_Compositional_Feature_Field_for_Editable_Scene_Rendering_With_Network-Inferred_CVPR_2023_paper.html" target="_blank">
Paper
</a>
</span>
</div>
<p class="desc">
We introduce metric learing for leveraging 2D network-inferred labels to obtain discriminating feature fields, leading to 3D segmentation and editing results.
</p>
</div>
</div>

<div class="publication">
<picture>
<source srcset="/media/CVPR2022-DGUNet.avif" type="image/avif" />
<source srcset="/media/CVPR2022-DGUNet.webp" type="image/webp" />
<img src="/media/CVPR2022-DGUNet.png" width="180" height="140" alt="Deep Generalized Unfolding Networks for Image Restoration" />
</picture>
<div>
<p class="title">Deep Generalized Unfolding Networks for Image Restoration</p>
<div class="authors">
<a href="https://github.com/MC-E" target="_blank">Chong Mou</a>,
<b>Qian Wang</b>,
<a href="https://jianzhang.tech/" target="_blank">Jian Zhang</a></div>
<p class="venue">CVPR, 2022</p>
<div class="links">
<span class="link">
<a href="https://openaccess.thecvf.com/content/CVPR2022/html/Mou_Deep_Generalized_Unfolding_Networks_for_Image_Restoration_CVPR_2022_paper.html" target="_blank">
Paper
</a>
</span>
<span class="link">
<a href="https://github.com/MC-E/Deep-Generalized-Unfolding-Networks-for-Image-Restoration" target="_blank">
Code
</a>
</span>
</div>
<p class="desc">
We integrate a gradient estimation strategy into the gradient descent step of the Proximal Gradient Descent algorithm, driving it to deal with complex real-world image degradation.
</p>
</div>
</div>

<div class="publication">
<picture>
<source srcset="/media/ACMMM2022-MSDPA.avif" type="image/avif" />
<source srcset="/media/ACMMM2022-MSDPA.webp" type="image/webp" />
<img src="/media/ACMMM2022-MSDPA.png" width="180" height="140" alt="More is better: Multi-source Dynamic Parsing Attention for Occluded Person Re-identification" />
</picture>
<div>
<p class="title">More is better: Multi-source Dynamic Parsing Attention for Occluded Person Re-identification</p>
<div class="authors">
<a href="https://cxh0519.github.io/" target="_blank">Xinhua Cheng*</a>,
<a href="https://mxjia.github.io/" target="_blank">Mengxi Jia*</a>,
<b>Qian Wang</b>,
<a href="https://jianzhang.tech/" target="_blank">Jian Zhang</a> (* equal contribution)
</div>
<p class="venue">ACM MM, 2022</p>
<div class="links">
<span class="link">
<a href="https://dl.acm.org/doi/abs/10.1145/3503161.3547819" target="_blank">
Paper
</a>
</span>
</div>
<p class="desc">
We introduce the multi-source knowledge ensemble in occluded re-ID to effective leverage external semantic cues learned from different domains.
</p>
</div>
</div>

<div class="publication">
<picture>
<source srcset="/media/TCSVT2022-VTB.avif" type="image/avif" />
<source srcset="/media/TCSVT2022-VTB.webp" type="image/webp" />
<img src="/media/TCSVT2022-VTB.png" width="180" height="140" alt="A Simple Visual-Textual Baseline for Pedestrian Attribute Recognition" />
</picture>
<div>
<p class="title">A Simple Visual-Textual Baseline for Pedestrian Attribute Recognition</p>
<div class="authors">
<a href="https://cxh0519.github.io/" target="_blank">Xinhua Cheng*</a>,
<a href="https://mxjia.github.io/" target="_blank">Mengxi Jia*</a>,
<b>Qian Wang</b>,
<a href="https://jianzhang.tech/" target="_blank">Jian Zhang</a> (* equal contribution)
</div>
<p class="venue">TCSVT, 2022</p>
<div class="links">
<span class="link">
<a href="https://ieeexplore.ieee.org/document/9588887" target="_blank">
Paper
</a>
</span>
<span class="link">
<a href="https://github.com/cxh0519/VTB" target="_blank">
Code
</a>
</span>
</div>
<p class="desc">
We model pedestrian attribute recognition as a multimodal problem and build a simple visual-textual baseline to captures the intra- and cross-modal correlations.
</p>
</div>
</div>

